proc main () = 
{
  true
  &&
  true
}


/* Start with undefined rhs */
mov ftmp4_0@uint64 _;
mov ftmp4_8@uint64 _;
mov ftmp4_16@uint64 _;
mov ftmp4_24@uint64 _;
mov ftmp4_32@uint64 _;
mov ftmp4_40@uint64 _;
mov ftmp4_48@uint64 _;
mov ftmp4_56@uint64 _;
mov ftmp4_64@uint64 _;
mov tmp2_0@uint128 _;
mov tmp2_16@uint128 _;
mov tmp2_32@uint128 _;
mov tmp2_48@uint128 _;
mov tmp2_64@uint128 _;
mov tmp2_80@uint128 _;
mov tmp2_96@uint128 _;
mov tmp2_112@uint128 _;
mov tmp2_128@uint128 _;
mov v344@uint64 _;
mov v466@uint64 _;
mov v470@uint64 _;
mov v577@uint64 _;
mov v580@uint64 _;
mov v598@uint64 _;
mov v631@uint64 _;
mov v633@uint64 _;
mov v645_0 _;
mov v645_8 _;
mov v664@uint64 _;
mov v678_0 _;
mov v678_8 _;
mov vect__168.214208_0 _;
mov vect__168.214208_8 _;
mov vect__168.21422_0 _;
mov vect__168.21422_8 _;
mov vect__168.214444_0 _;
mov vect__168.214444_8 _;
mov vect__168.214554_0 _;
mov vect__168.214554_8 _;
mov vect__170.216210_0 _;
mov vect__170.216210_8 _;
mov vect__170.216217_0 _;
mov vect__170.216217_8 _;
mov vect__170.216438_0 _;
mov vect__170.216438_8 _;
mov vect__170.216569_0 _;
mov vect__170.216569_8 _;
mov vect__179.227165_0 _;
mov vect__179.227165_8 _;
mov vect__179.227201_0 _;
mov vect__179.227201_8 _;
mov vect__179.227249_0 _;
mov vect__179.227249_8 _;
mov vect__179.227574_0 _;
mov vect__179.227574_8 _;
mov vect__190.240279_0 _;
mov vect__190.240279_8 _;
mov vect__190.240565_0 _;
mov vect__190.240565_8 _;
mov vect__190.240696_0 _;
mov vect__190.240696_8 _;
mov vect__190.240711_0 _;
mov vect__190.240711_8 _;
mov vect__192.242281_0 _;
mov vect__192.242281_8 _;
mov vect__192.242555_0 _;
mov vect__192.242555_8 _;
mov vect__192.242698_0 _;
mov vect__192.242698_8 _;
mov vect__192.242713_0 _;
mov vect__192.242713_8 _;
mov vect__201.253611_0 _;
mov vect__201.253611_8 _;
mov vect__201.253726_0 _;
mov vect__201.253726_8 _;
mov vect__201.253741_0 _;
mov vect__201.253741_8 _;
mov vect__201.253756_0 _;
mov vect__201.253756_8 _;
mov vect__212.266644_0 _;
mov vect__212.266644_8 _;
mov vect__212.266771_0 _;
mov vect__212.266771_8 _;
mov vect__212.266786_0 _;
mov vect__212.266786_8 _;
mov vect__212.266801_0 _;
mov vect__212.266801_8 _;
mov vect__214.268647_0 _;
mov vect__214.268647_8 _;
mov vect__214.268773_0 _;
mov vect__214.268773_8 _;
mov vect__214.268788_0 _;
mov vect__214.268788_8 _;
mov vect__214.268803_0 _;
mov vect__214.268803_8 _;
mov vect__223.279677_0 _;
mov vect__223.279677_8 _;
mov vect__223.279816_0 _;
mov vect__223.279816_8 _;
mov vect__223.279831_0 _;
mov vect__223.279831_8 _;
mov vect__223.279846_0 _;
mov vect__223.279846_8 _;
mov x119_0@uint64 _;
mov x119_8@uint64 _;
mov x119_16@uint64 _;
mov x119_24@uint64 _;
mov x119_32@uint64 _;
mov x119_40@uint64 _;
mov x119_48@uint64 _;
mov x119_56@uint64 _;
mov x119_64@uint64 _;
mov x232_0@uint64 _;
mov x232_8@uint64 _;
mov x232_16@uint64 _;
mov x232_24@uint64 _;
mov x232_32@uint64 _;
mov x232_40@uint64 _;
mov x232_48@uint64 _;
mov x232_56@uint64 _;
mov x232_64@uint64 _;
mov x_equal37@uint64 _;
mov x_out_0@uint64 _;
mov x_out_8@uint64 _;
mov x_out_16@uint64 _;
mov x_out_24@uint64 _;
mov x_out_32@uint64 _;
mov x_out_40@uint64 _;
mov x_out_48@uint64 _;
mov x_out_56@uint64 _;
mov x_out_64@uint64 _;
mov y129_0@uint64 _;
mov y129_8@uint64 _;
mov y129_16@uint64 _;
mov y129_24@uint64 _;
mov y129_32@uint64 _;
mov y129_40@uint64 _;
mov y129_48@uint64 _;
mov y129_56@uint64 _;
mov y129_64@uint64 _;
mov y242_0@uint64 _;
mov y242_8@uint64 _;
mov y242_16@uint64 _;
mov y242_24@uint64 _;
mov y242_32@uint64 _;
mov y242_40@uint64 _;
mov y242_48@uint64 _;
mov y242_56@uint64 _;
mov y242_64@uint64 _;
mov y_equal47@uint64 _;
mov y_out_0@uint64 _;
mov y_out_8@uint64 _;
mov y_out_16@uint64 _;
mov y_out_24@uint64 _;
mov y_out_32@uint64 _;
mov y_out_40@uint64 _;
mov y_out_48@uint64 _;
mov y_out_56@uint64 _;
mov y_out_64@uint64 _;
mov z18_0@uint64 _;
mov z18_8@uint64 _;
mov z18_16@uint64 _;
mov z18_24@uint64 _;
mov z18_32@uint64 _;
mov z18_40@uint64 _;
mov z18_48@uint64 _;
mov z18_56@uint64 _;
mov z18_64@uint64 _;
mov z1_is_zero10@uint64 _;
mov z211_0@uint64 _;
mov z211_8@uint64 _;
mov z211_16@uint64 _;
mov z211_24@uint64 _;
mov z211_32@uint64 _;
mov z211_40@uint64 _;
mov z211_48@uint64 _;
mov z211_56@uint64 _;
mov z211_64@uint64 _;
mov z2_is_zero13@uint64 _;
mov z_out_0@uint64 _;
mov z_out_8@uint64 _;
mov z_out_16@uint64 _;
mov z_out_24@uint64 _;
mov z_out_32@uint64 _;
mov z_out_40@uint64 _;
mov z_out_48@uint64 _;
mov z_out_56@uint64 _;
mov z_out_64@uint64 _;
/* End with undefined rhs */



/* BB's index is 2 */

/* z1_is_zero_10 = felem_is_zero (z1_8(D)); */
/* TODO: skipped, GIMPLE_CALL doesn't use internal or builtin function, inline function or self translte */
/* z2_is_zero_13 = felem_is_zero (z2_11(D)); */
/* TODO: skipped, GIMPLE_CALL doesn't use internal or builtin function, inline function or self translte */
/* felem_square (&tmp, z1_8(D)); */
/* TODO: skipped, GIMPLE_CALL doesn't use internal or builtin function, inline function or self translte */
/* felem_reduce (&ftmp, &tmp); */
/* TODO: skipped, GIMPLE_CALL doesn't use internal or builtin function, inline function or self translte */
/* if (mixed_16(D) == 0) */
/* Fix: Skip GIMPLE_COND, need to take it into consideration */
/* Note: True label: <bb 3>, False label: <bb 4> */

/* BB's index is 3 */

/* felem_square (&tmp, z2_11(D)); */
/* TODO: skipped, GIMPLE_CALL doesn't use internal or builtin function, inline function or self translte */
/* felem_reduce (&ftmp2, &tmp); */
/* TODO: skipped, GIMPLE_CALL doesn't use internal or builtin function, inline function or self translte */
/* felem_mul (&tmp, x1_19(D), &ftmp2); */
/* TODO: skipped, GIMPLE_CALL doesn't use internal or builtin function, inline function or self translte */
/* felem_reduce (&ftmp3, &tmp); */
/* TODO: skipped, GIMPLE_CALL doesn't use internal or builtin function, inline function or self translte */
/* vect__46.286_544 = MEM[(const limb *)z1_8(D)]; */
mov vect__46.286544_0 z18_0;
mov vect__46.286544_8 z18_8;
/* vect__46.287_424 = MEM[(const limb *)z1_8(D) + 16B]; */
mov vect__46.287424_0 z18_16;
mov vect__46.287424_8 z18_24;
/* vect__46.288_450 = MEM[(const limb *)z1_8(D) + 32B]; */
mov vect__46.288450_0 z18_32;
mov vect__46.288450_8 z18_40;
/* vect__46.289_537 = MEM[(const limb *)z1_8(D) + 48B]; */
mov vect__46.289537_0 z18_48;
mov vect__46.289537_8 z18_56;
/* _97 = MEM[(const limb *)z1_8(D) + 64B]; */
mov v97 z18_64;
/* vect__339.292_225 = MEM[(const limb *)z2_11(D)]; */
mov vect__339.292225_0 z211_0;
mov vect__339.292225_8 z211_8;
/* vect__339.293_223 = MEM[(const limb *)z2_11(D) + 16B]; */
mov vect__339.293223_0 z211_16;
mov vect__339.293223_8 z211_24;
/* vect__339.294_561 = MEM[(const limb *)z2_11(D) + 32B]; */
mov vect__339.294561_0 z211_32;
mov vect__339.294561_8 z211_40;
/* vect__339.295_621 = MEM[(const limb *)z2_11(D) + 48B]; */
mov vect__339.295621_0 z211_48;
mov vect__339.295621_8 z211_56;
/* vect__340.296_412 = vect__339.292_225 + vect__46.286_544; */
uadd vect__340.296412_0 vect__339.292225_0 vect__46.286544_0;
uadd vect__340.296412_8 vect__339.292225_8 vect__46.286544_8;
/* vect__340.296_48 = vect__339.293_223 + vect__46.287_424; */
uadd vect__340.29648_0 vect__339.293223_0 vect__46.287424_0;
uadd vect__340.29648_8 vect__339.293223_8 vect__46.287424_8;
/* vect__340.296_589 = vect__46.288_450 + vect__339.294_561; */
uadd vect__340.296589_0 vect__46.288450_0 vect__339.294561_0;
uadd vect__340.296589_8 vect__46.288450_8 vect__339.294561_8;
/* vect__340.296_458 = vect__46.289_537 + vect__339.295_621; */
uadd vect__340.296458_0 vect__46.289537_0 vect__339.295621_0;
uadd vect__340.296458_8 vect__46.289537_8 vect__339.295621_8;
/* MEM[(limb *)&ftmp5] = vect__340.296_412; */
mov ftmp5_0 vect__340.296412_0;
mov ftmp5_8 vect__340.296412_8;
/* MEM[(limb *)&ftmp5 + 16B] = vect__340.296_48; */
mov ftmp5_16 vect__340.29648_0;
mov ftmp5_24 vect__340.29648_8;
/* MEM[(limb *)&ftmp5 + 32B] = vect__340.296_589; */
mov ftmp5_32 vect__340.296589_0;
mov ftmp5_40 vect__340.296589_8;
/* MEM[(limb *)&ftmp5 + 48B] = vect__340.296_458; */
mov ftmp5_48 vect__340.296458_0;
mov ftmp5_56 vect__340.296458_8;
/* _363 = MEM[(const limb *)z2_11(D) + 64B]; */
mov v363 z211_64;
/* _364 = _97 + _363; */
uadd v364 v97 v363;
/* MEM[(limb *)&ftmp5 + 64B] = _364; */
mov ftmp5_64 v364;
/* felem_square (&tmp, &ftmp5); */
/* TODO: skipped, GIMPLE_CALL doesn't use internal or builtin function, inline function or self translte */
/* felem_diff_128_64 (&tmp, &ftmp); */
/* TODO: skipped, GIMPLE_CALL doesn't use internal or builtin function, inline function or self translte */
/* felem_diff_128_64 (&tmp, &ftmp2); */
/* TODO: skipped, GIMPLE_CALL doesn't use internal or builtin function, inline function or self translte */
/* felem_reduce (&ftmp5, &tmp); */
/* TODO: skipped, GIMPLE_CALL doesn't use internal or builtin function, inline function or self translte */
/* felem_mul (&tmp, &ftmp2, z2_11(D)); */
/* TODO: skipped, GIMPLE_CALL doesn't use internal or builtin function, inline function or self translte */
/* felem_reduce (&ftmp2, &tmp); */
/* TODO: skipped, GIMPLE_CALL doesn't use internal or builtin function, inline function or self translte */
/* felem_mul (&tmp, y1_29(D), &ftmp2); */
/* TODO: skipped, GIMPLE_CALL doesn't use internal or builtin function, inline function or self translte */
/* felem_reduce (&ftmp6, &tmp); */
/* TODO: skipped, GIMPLE_CALL doesn't use internal or builtin function, inline function or self translte */
/* goto <bb 5> */
/* TODO: unconditional jump */

/* BB's index is 4 */

/* _125 = *x1_19(D); */
mov v125 x119_0;
/* MEM[(limb *)&ftmp3] = _125; */
mov ftmp3_0 v125;
/* _126 = MEM[(const limb *)x1_19(D) + 8B]; */
mov v126 x119_8;
/* MEM[(limb *)&ftmp3 + 8B] = _126; */
mov ftmp3_8 v126;
/* _127 = MEM[(const limb *)x1_19(D) + 16B]; */
mov v127 x119_16;
/* MEM[(limb *)&ftmp3 + 16B] = _127; */
mov ftmp3_16 v127;
/* _128 = MEM[(const limb *)x1_19(D) + 24B]; */
mov v128 x119_24;
/* MEM[(limb *)&ftmp3 + 24B] = _128; */
mov ftmp3_24 v128;
/* _129 = MEM[(const limb *)x1_19(D) + 32B]; */
mov v129 x119_32;
/* MEM[(limb *)&ftmp3 + 32B] = _129; */
mov ftmp3_32 v129;
/* _130 = MEM[(const limb *)x1_19(D) + 40B]; */
mov v130 x119_40;
/* MEM[(limb *)&ftmp3 + 40B] = _130; */
mov ftmp3_40 v130;
/* _131 = MEM[(const limb *)x1_19(D) + 48B]; */
mov v131 x119_48;
/* MEM[(limb *)&ftmp3 + 48B] = _131; */
mov ftmp3_48 v131;
/* _132 = MEM[(const limb *)x1_19(D) + 56B]; */
mov v132 x119_56;
/* MEM[(limb *)&ftmp3 + 56B] = _132; */
mov ftmp3_56 v132;
/* _133 = MEM[(const limb *)x1_19(D) + 64B]; */
mov v133 x119_64;
/* MEM[(limb *)&ftmp3 + 64B] = _133; */
mov ftmp3_64 v133;
/* _107 = *z1_8(D); */
mov v107 z18_0;
/* _108 = _107 * 2; */
umul v108 v107 0x2@uint64;
/* MEM[(limb *)&ftmp5] = _108; */
mov ftmp5_0 v108;
/* _109 = MEM[(const limb *)z1_8(D) + 8B]; */
mov v109 z18_8;
/* _110 = _109 * 2; */
umul v110 v109 0x2@uint64;
/* MEM[(limb *)&ftmp5 + 8B] = _110; */
mov ftmp5_8 v110;
/* _111 = MEM[(const limb *)z1_8(D) + 16B]; */
mov v111 z18_16;
/* _112 = _111 * 2; */
umul v112 v111 0x2@uint64;
/* MEM[(limb *)&ftmp5 + 16B] = _112; */
mov ftmp5_16 v112;
/* _113 = MEM[(const limb *)z1_8(D) + 24B]; */
mov v113 z18_24;
/* _114 = _113 * 2; */
umul v114 v113 0x2@uint64;
/* MEM[(limb *)&ftmp5 + 24B] = _114; */
mov ftmp5_24 v114;
/* _115 = MEM[(const limb *)z1_8(D) + 32B]; */
mov v115 z18_32;
/* _116 = _115 * 2; */
umul v116 v115 0x2@uint64;
/* MEM[(limb *)&ftmp5 + 32B] = _116; */
mov ftmp5_32 v116;
/* _117 = MEM[(const limb *)z1_8(D) + 40B]; */
mov v117 z18_40;
/* _118 = _117 * 2; */
umul v118 v117 0x2@uint64;
/* MEM[(limb *)&ftmp5 + 40B] = _118; */
mov ftmp5_40 v118;
/* _119 = MEM[(const limb *)z1_8(D) + 48B]; */
mov v119 z18_48;
/* _120 = _119 * 2; */
umul v120 v119 0x2@uint64;
/* MEM[(limb *)&ftmp5 + 48B] = _120; */
mov ftmp5_48 v120;
/* _121 = MEM[(const limb *)z1_8(D) + 56B]; */
mov v121 z18_56;
/* _122 = _121 * 2; */
umul v122 v121 0x2@uint64;
/* MEM[(limb *)&ftmp5 + 56B] = _122; */
mov ftmp5_56 v122;
/* _123 = MEM[(const limb *)z1_8(D) + 64B]; */
mov v123 z18_64;
/* _124 = _123 * 2; */
umul v124 v123 0x2@uint64;
/* MEM[(limb *)&ftmp5 + 64B] = _124; */
mov ftmp5_64 v124;
/* _98 = *y1_29(D); */
mov v98 y129_0;
/* MEM[(limb *)&ftmp6] = _98; */
mov ftmp6_0 v98;
/* _99 = MEM[(const limb *)y1_29(D) + 8B]; */
mov v99 y129_8;
/* MEM[(limb *)&ftmp6 + 8B] = _99; */
mov ftmp6_8 v99;
/* _100 = MEM[(const limb *)y1_29(D) + 16B]; */
mov v100 y129_16;
/* MEM[(limb *)&ftmp6 + 16B] = _100; */
mov ftmp6_16 v100;
/* _101 = MEM[(const limb *)y1_29(D) + 24B]; */
mov v101 y129_24;
/* MEM[(limb *)&ftmp6 + 24B] = _101; */
mov ftmp6_24 v101;
/* _102 = MEM[(const limb *)y1_29(D) + 32B]; */
mov v102 y129_32;
/* MEM[(limb *)&ftmp6 + 32B] = _102; */
mov ftmp6_32 v102;
/* _103 = MEM[(const limb *)y1_29(D) + 40B]; */
mov v103 y129_40;
/* MEM[(limb *)&ftmp6 + 40B] = _103; */
mov ftmp6_40 v103;
/* _104 = MEM[(const limb *)y1_29(D) + 48B]; */
mov v104 y129_48;
/* MEM[(limb *)&ftmp6 + 48B] = _104; */
mov ftmp6_48 v104;
/* _105 = MEM[(const limb *)y1_29(D) + 56B]; */
mov v105 y129_56;
/* MEM[(limb *)&ftmp6 + 56B] = _105; */
mov ftmp6_56 v105;
/* _106 = MEM[(const limb *)y1_29(D) + 64B]; */
mov v106 y129_64;
/* MEM[(limb *)&ftmp6 + 64B] = _106; */
mov ftmp6_64 v106;
/* goto <bb 5> */
/* TODO: unconditional jump */

/* BB's index is 5 */

/* .MEM_5 = PHI <.MEM_31(3), .MEM_337(4)> */
/* felem_mul (&tmp, x2_32(D), &ftmp); */
/* TODO: skipped, GIMPLE_CALL doesn't use internal or builtin function, inline function or self translte */
/* felem_diff_128_64 (&tmp, &ftmp3); */
/* TODO: skipped, GIMPLE_CALL doesn't use internal or builtin function, inline function or self translte */
/* felem_reduce (&ftmp4, &tmp); */
/* TODO: skipped, GIMPLE_CALL doesn't use internal or builtin function, inline function or self translte */
/* x_equal_37 = felem_is_zero (&ftmp4); */
/* TODO: skipped, GIMPLE_CALL doesn't use internal or builtin function, inline function or self translte */
/* felem_mul (&tmp, &ftmp5, &ftmp4); */
/* TODO: skipped, GIMPLE_CALL doesn't use internal or builtin function, inline function or self translte */
/* felem_reduce (&z_out, &tmp); */
/* TODO: skipped, GIMPLE_CALL doesn't use internal or builtin function, inline function or self translte */
/* felem_mul (&tmp, &ftmp, z1_8(D)); */
/* TODO: skipped, GIMPLE_CALL doesn't use internal or builtin function, inline function or self translte */
/* felem_reduce (&ftmp, &tmp); */
/* TODO: skipped, GIMPLE_CALL doesn't use internal or builtin function, inline function or self translte */
/* felem_mul (&tmp, y2_42(D), &ftmp); */
/* TODO: skipped, GIMPLE_CALL doesn't use internal or builtin function, inline function or self translte */
/* felem_diff_128_64 (&tmp, &ftmp6); */
/* TODO: skipped, GIMPLE_CALL doesn't use internal or builtin function, inline function or self translte */
/* felem_reduce (&ftmp5, &tmp); */
/* TODO: skipped, GIMPLE_CALL doesn't use internal or builtin function, inline function or self translte */
/* y_equal_47 = felem_is_zero (&ftmp5); */
/* TODO: skipped, GIMPLE_CALL doesn't use internal or builtin function, inline function or self translte */
/* _365 = MEM[(limb *)&ftmp5]; */
mov v365 ftmp5_0;
/* _366 = _365 * 2; */
umul v366 v365 0x2@uint64;
/* MEM[(limb *)&ftmp5] = _366; */
mov ftmp5_0 v366;
/* _367 = MEM[(limb *)&ftmp5 + 8B]; */
mov v367 ftmp5_8;
/* _368 = _367 * 2; */
umul v368 v367 0x2@uint64;
/* MEM[(limb *)&ftmp5 + 8B] = _368; */
mov ftmp5_8 v368;
/* _369 = MEM[(limb *)&ftmp5 + 16B]; */
mov v369 ftmp5_16;
/* _370 = _369 * 2; */
umul v370 v369 0x2@uint64;
/* MEM[(limb *)&ftmp5 + 16B] = _370; */
mov ftmp5_16 v370;
/* _371 = MEM[(limb *)&ftmp5 + 24B]; */
mov v371 ftmp5_24;
/* _372 = _371 * 2; */
umul v372 v371 0x2@uint64;
/* MEM[(limb *)&ftmp5 + 24B] = _372; */
mov ftmp5_24 v372;
/* _373 = MEM[(limb *)&ftmp5 + 32B]; */
mov v373 ftmp5_32;
/* _374 = _373 * 2; */
umul v374 v373 0x2@uint64;
/* MEM[(limb *)&ftmp5 + 32B] = _374; */
mov ftmp5_32 v374;
/* _375 = MEM[(limb *)&ftmp5 + 40B]; */
mov v375 ftmp5_40;
/* _376 = _375 * 2; */
umul v376 v375 0x2@uint64;
/* MEM[(limb *)&ftmp5 + 40B] = _376; */
mov ftmp5_40 v376;
/* _377 = MEM[(limb *)&ftmp5 + 48B]; */
mov v377 ftmp5_48;
/* _378 = _377 * 2; */
umul v378 v377 0x2@uint64;
/* MEM[(limb *)&ftmp5 + 48B] = _378; */
mov ftmp5_48 v378;
/* _379 = MEM[(limb *)&ftmp5 + 56B]; */
mov v379 ftmp5_56;
/* _380 = _379 * 2; */
umul v380 v379 0x2@uint64;
/* MEM[(limb *)&ftmp5 + 56B] = _380; */
mov ftmp5_56 v380;
/* _381 = MEM[(limb *)&ftmp5 + 64B]; */
mov v381 ftmp5_64;
/* _382 = _381 * 2; */
umul v382 v381 0x2@uint64;
/* MEM[(limb *)&ftmp5 + 64B] = _382; */
mov ftmp5_64 v382;
/* _1 = x_equal_37 != 0; */
subb lt_value dontcare x_equal37 0x0@uint64;
subb gt_value dontcare value 0x0@uint64 x_equal37;
or uint1 v1 lt_value gt_value;
vpc v1@uint8 v1@uint1;
/* _2 = y_equal_47 != 0; */
subb lt_value dontcare y_equal47 0x0@uint64;
subb gt_value dontcare value 0x0@uint64 y_equal47;
or uint1 v2 lt_value gt_value;
vpc v2@uint8 v2@uint1;
/* _3 = _1 & _2; */
/* TODO: Bitwise And/Or , may need assert and assume */
and uint8 v3 v1 v2;
/* if (_3 != 0) */
/* Fix: Skip GIMPLE_COND, need to take it into consideration */
/* Note: True label: <bb 6>, False label: <bb 8> */

/* BB's index is 6 */

/* _4 = z1_is_zero_10 | z2_is_zero_13; */
/* TODO: Bitwise And/Or , may need assert and assume */
or uint64 v4 z1_is_zero10 z2_is_zero13;
/* if (_4 == 0) */
/* Fix: Skip GIMPLE_COND, need to take it into consideration */
/* Note: True label: <bb 7>, False label: <bb 8> */

/* BB's index is 7 */

/* point_double (x3_49(D), y3_50(D), z3_51(D), x1_19(D), y1_29(D), z1_8(D)); */
/* TODO: skipped, GIMPLE_CALL doesn't use internal or builtin function, inline function or self translte */
/* ftmp ={v} {CLOBBER}; */
/* TODO: Skip translation for constructor */
/* ftmp2 ={v} {CLOBBER}; */
/* TODO: Skip translation for constructor */
/* ftmp3 ={v} {CLOBBER}; */
/* TODO: Skip translation for constructor */
/* ftmp4 ={v} {CLOBBER}; */
/* TODO: Skip translation for constructor */
/* ftmp5 ={v} {CLOBBER}; */
/* TODO: Skip translation for constructor */
/* ftmp6 ={v} {CLOBBER}; */
/* TODO: Skip translation for constructor */
/* x_out ={v} {CLOBBER}; */
/* TODO: Skip translation for constructor */
/* y_out ={v} {CLOBBER}; */
/* TODO: Skip translation for constructor */
/* z_out ={v} {CLOBBER}; */
/* TODO: Skip translation for constructor */
/* tmp ={v} {CLOBBER}; */
/* TODO: Skip translation for constructor */
/* tmp2 ={v} {CLOBBER}; */
/* TODO: Skip translation for constructor */
/* goto <bb 9> */
/* TODO: unconditional jump */

/* BB's index is 8 */

/* _236 = MEM[(const limb *)&ftmp4]; */
mov v236 ftmp4_0;
/* _237 = MEM[(const limb *)&ftmp4 + 8B]; */
mov v237 ftmp4_8;
/* _238 = MEM[(const limb *)&ftmp4 + 16B]; */
mov v238 ftmp4_16;
/* _239 = MEM[(const limb *)&ftmp4 + 24B]; */
mov v239 ftmp4_24;
/* _240 = MEM[(const limb *)&ftmp4 + 32B]; */
mov v240 ftmp4_32;
/* _241 = MEM[(const limb *)&ftmp4 + 40B]; */
mov v241 ftmp4_40;
/* _242 = MEM[(const limb *)&ftmp4 + 48B]; */
mov v242 ftmp4_48;
/* _243 = MEM[(const limb *)&ftmp4 + 56B]; */
mov v243 ftmp4_56;
/* _244 = MEM[(const limb *)&ftmp4 + 64B]; */
mov v244 ftmp4_64;
/* _457 = _236 * 2; */
umul v457 v236 0x2@uint64;
/* MEM[(limb *)&ftmp] = _457; */
mov ftmp_0 v457;
/* _459 = _237 * 2; */
umul v459 v237 0x2@uint64;
/* MEM[(limb *)&ftmp + 8B] = _459; */
mov ftmp_8 v459;
/* _461 = _238 * 2; */
umul v461 v238 0x2@uint64;
/* MEM[(limb *)&ftmp + 16B] = _461; */
mov ftmp_16 v461;
/* _463 = _239 * 2; */
umul v463 v239 0x2@uint64;
/* MEM[(limb *)&ftmp + 24B] = _463; */
mov ftmp_24 v463;
/* _465 = _240 * 2; */
umul v465 v240 0x2@uint64;
/* MEM[(limb *)&ftmp + 32B] = _465; */
mov ftmp_32 v465;
/* _467 = _241 * 2; */
umul v467 v241 0x2@uint64;
/* MEM[(limb *)&ftmp + 40B] = _467; */
mov ftmp_40 v467;
/* _469 = _242 * 2; */
umul v469 v242 0x2@uint64;
/* MEM[(limb *)&ftmp + 48B] = _469; */
mov ftmp_48 v469;
/* _471 = _243 * 2; */
umul v471 v243 0x2@uint64;
/* MEM[(limb *)&ftmp + 56B] = _471; */
mov ftmp_56 v471;
/* _473 = _244 * 2; */
umul v473 v244 0x2@uint64;
/* MEM[(limb *)&ftmp + 64B] = _473; */
mov ftmp_64 v473;
/* felem_square (&tmp, &ftmp); */
/* TODO: skipped, GIMPLE_CALL doesn't use internal or builtin function, inline function or self translte */
/* felem_reduce (&ftmp, &tmp); */
/* TODO: skipped, GIMPLE_CALL doesn't use internal or builtin function, inline function or self translte */
/* felem_mul (&tmp, &ftmp4, &ftmp); */
/* TODO: skipped, GIMPLE_CALL doesn't use internal or builtin function, inline function or self translte */
/* felem_reduce (&ftmp2, &tmp); */
/* TODO: skipped, GIMPLE_CALL doesn't use internal or builtin function, inline function or self translte */
/* felem_mul (&tmp, &ftmp3, &ftmp); */
/* TODO: skipped, GIMPLE_CALL doesn't use internal or builtin function, inline function or self translte */
/* felem_reduce (&ftmp4, &tmp); */
/* TODO: skipped, GIMPLE_CALL doesn't use internal or builtin function, inline function or self translte */
/* felem_square (&tmp, &ftmp5); */
/* TODO: skipped, GIMPLE_CALL doesn't use internal or builtin function, inline function or self translte */
/* felem_diff_128_64 (&tmp, &ftmp2); */
/* TODO: skipped, GIMPLE_CALL doesn't use internal or builtin function, inline function or self translte */
/* _227 = MEM[(const limb *)&ftmp4]; */
mov v227 ftmp4_0;
/* MEM[(limb *)&ftmp3] = _227; */
mov ftmp3_0 v227;
/* _228 = MEM[(const limb *)&ftmp4 + 8B]; */
mov v228 ftmp4_8;
/* MEM[(limb *)&ftmp3 + 8B] = _228; */
mov ftmp3_8 v228;
/* _229 = MEM[(const limb *)&ftmp4 + 16B]; */
mov v229 ftmp4_16;
/* MEM[(limb *)&ftmp3 + 16B] = _229; */
mov ftmp3_16 v229;
/* _230 = MEM[(const limb *)&ftmp4 + 24B]; */
mov v230 ftmp4_24;
/* MEM[(limb *)&ftmp3 + 24B] = _230; */
mov ftmp3_24 v230;
/* _231 = MEM[(const limb *)&ftmp4 + 32B]; */
mov v231 ftmp4_32;
/* MEM[(limb *)&ftmp3 + 32B] = _231; */
mov ftmp3_32 v231;
/* _232 = MEM[(const limb *)&ftmp4 + 40B]; */
mov v232 ftmp4_40;
/* MEM[(limb *)&ftmp3 + 40B] = _232; */
mov ftmp3_40 v232;
/* _233 = MEM[(const limb *)&ftmp4 + 48B]; */
mov v233 ftmp4_48;
/* MEM[(limb *)&ftmp3 + 48B] = _233; */
mov ftmp3_48 v233;
/* _234 = MEM[(const limb *)&ftmp4 + 56B]; */
mov v234 ftmp4_56;
/* MEM[(limb *)&ftmp3 + 56B] = _234; */
mov ftmp3_56 v234;
/* _235 = MEM[(const limb *)&ftmp4 + 64B]; */
mov v235 ftmp4_64;
/* MEM[(limb *)&ftmp3 + 64B] = _235; */
mov ftmp3_64 v235;
/* _439 = _227 * 2; */
umul v439 v227 0x2@uint64;
/* MEM[(limb *)&ftmp4] = _439; */
mov ftmp4_0 v439;
/* _441 = _228 * 2; */
umul v441 v228 0x2@uint64;
/* MEM[(limb *)&ftmp4 + 8B] = _441; */
mov ftmp4_8 v441;
/* _443 = _229 * 2; */
umul v443 v229 0x2@uint64;
/* MEM[(limb *)&ftmp4 + 16B] = _443; */
mov ftmp4_16 v443;
/* _445 = _230 * 2; */
umul v445 v230 0x2@uint64;
/* MEM[(limb *)&ftmp4 + 24B] = _445; */
mov ftmp4_24 v445;
/* _447 = _231 * 2; */
umul v447 v231 0x2@uint64;
/* MEM[(limb *)&ftmp4 + 32B] = _447; */
mov ftmp4_32 v447;
/* _449 = _232 * 2; */
umul v449 v232 0x2@uint64;
/* MEM[(limb *)&ftmp4 + 40B] = _449; */
mov ftmp4_40 v449;
/* _451 = _233 * 2; */
umul v451 v233 0x2@uint64;
/* MEM[(limb *)&ftmp4 + 48B] = _451; */
mov ftmp4_48 v451;
/* _453 = _234 * 2; */
umul v453 v234 0x2@uint64;
/* MEM[(limb *)&ftmp4 + 56B] = _453; */
mov ftmp4_56 v453;
/* _455 = _235 * 2; */
umul v455 v235 0x2@uint64;
/* MEM[(limb *)&ftmp4 + 64B] = _455; */
mov ftmp4_64 v455;
/* felem_diff_128_64 (&tmp, &ftmp4); */
/* TODO: skipped, GIMPLE_CALL doesn't use internal or builtin function, inline function or self translte */
/* felem_reduce (&x_out, &tmp); */
/* TODO: skipped, GIMPLE_CALL doesn't use internal or builtin function, inline function or self translte */
/* vect__402.301_264 = MEM[(limb *)&ftmp3]; */
mov vect__402.301264_0 ftmp3_0;
mov vect__402.301264_8 ftmp3_8;
/* vect__402.302_162 = MEM[(limb *)&ftmp3 + 16B]; */
mov vect__402.302162_0 ftmp3_16;
mov vect__402.302162_8 ftmp3_24;
/* vect__402.303_182 = MEM[(limb *)&ftmp3 + 32B]; */
mov vect__402.303182_0 ftmp3_32;
mov vect__402.303182_8 ftmp3_40;
/* vect__402.304_180 = MEM[(limb *)&ftmp3 + 48B]; */
mov vect__402.304180_0 ftmp3_48;
mov vect__402.304180_8 ftmp3_56;
/* vect__403.308_560 = MEM[(const limb *)&x_out]; */
mov vect__403.308560_0 x_out_0;
mov vect__403.308560_8 x_out_8;
/* vect__403.309_76 = MEM[(const limb *)&x_out + 16B]; */
mov vect__403.30976_0 x_out_16;
mov vect__403.30976_8 x_out_24;
/* vect__403.310_552 = MEM[(const limb *)&x_out + 32B]; */
mov vect__403.310552_0 x_out_32;
mov vect__403.310552_8 x_out_40;
/* vect__403.311_566 = MEM[(const limb *)&x_out + 48B]; */
mov vect__403.311566_0 x_out_48;
mov vect__403.311566_8 x_out_56;
/* vect__259.305_198 = vect__402.301_264 + { 4611686018427387872, 4611686018427387888 }; */
uadd vect__259.305198_0 vect__402.301264_0 0x3fffffffffffffe0@uint64;
uadd vect__259.305198_8 vect__402.301264_8 0x3ffffffffffffff0@uint64;
/* vect__259.305_197 = vect__402.302_162 + { 4611686018427387888, 4611686018427387888 }; */
uadd vect__259.305197_0 vect__402.302162_0 0x3ffffffffffffff0@uint64;
uadd vect__259.305197_8 vect__402.302162_8 0x3ffffffffffffff0@uint64;
/* vect__259.305_196 = vect__402.303_182 + { 4611686018427387888, 4611686018427387888 }; */
uadd vect__259.305196_0 vect__402.303182_0 0x3ffffffffffffff0@uint64;
uadd vect__259.305196_8 vect__402.303182_8 0x3ffffffffffffff0@uint64;
/* vect__259.305_195 = vect__402.304_180 + { 4611686018427387888, 4611686018427387888 }; */
uadd vect__259.305195_0 vect__402.304180_0 0x3ffffffffffffff0@uint64;
uadd vect__259.305195_8 vect__402.304180_8 0x3ffffffffffffff0@uint64;
/* vect__405.312_535 = vect__259.305_198 - vect__403.308_560; */
usub vect__405.312535_0 vect__259.305198_0 vect__403.308560_0;
usub vect__405.312535_8 vect__259.305198_8 vect__403.308560_8;
/* vect__405.312_708 = vect__259.305_197 - vect__403.309_76; */
usub vect__405.312708_0 vect__259.305197_0 vect__403.30976_0;
usub vect__405.312708_8 vect__259.305197_8 vect__403.30976_8;
/* vect__405.312_707 = vect__259.305_196 - vect__403.310_552; */
usub vect__405.312707_0 vect__259.305196_0 vect__403.310552_0;
usub vect__405.312707_8 vect__259.305196_8 vect__403.310552_8;
/* vect__405.312_706 = vect__259.305_195 - vect__403.311_566; */
usub vect__405.312706_0 vect__259.305195_0 vect__403.311566_0;
usub vect__405.312706_8 vect__259.305195_8 vect__403.311566_8;
/* MEM[(limb *)&ftmp3] = vect__405.312_535; */
mov ftmp3_0 vect__405.312535_0;
mov ftmp3_8 vect__405.312535_8;
/* MEM[(limb *)&ftmp3 + 16B] = vect__405.312_708; */
mov ftmp3_16 vect__405.312708_0;
mov ftmp3_24 vect__405.312708_8;
/* MEM[(limb *)&ftmp3 + 32B] = vect__405.312_707; */
mov ftmp3_32 vect__405.312707_0;
mov ftmp3_40 vect__405.312707_8;
/* MEM[(limb *)&ftmp3 + 48B] = vect__405.312_706; */
mov ftmp3_48 vect__405.312706_0;
mov ftmp3_56 vect__405.312706_8;
/* _434 = MEM[(limb *)&ftmp3 + 64B]; */
mov v434 ftmp3_64;
/* _435 = MEM[(const limb *)&x_out + 64B]; */
mov v435 x_out_64;
/* _251 = _434 + 4611686018427387888; */
uadd v251 v434 0x3ffffffffffffff0@uint64;
/* _437 = _251 - _435; */
usub v437 v251 v435;
/* MEM[(limb *)&ftmp3 + 64B] = _437; */
mov ftmp3_64 v437;
/* felem_mul (&tmp, &ftmp5, &ftmp3); */
/* TODO: skipped, GIMPLE_CALL doesn't use internal or builtin function, inline function or self translte */
/* felem_mul (&tmp2, &ftmp6, &ftmp2); */
/* TODO: skipped, GIMPLE_CALL doesn't use internal or builtin function, inline function or self translte */
/* _383 = MEM[(uint128_t *)&tmp2]; */
mov v383 tmp2_0;
/* _385 = _383 * 2; */
umul v385 v383 0x2@uint128;
/* MEM[(uint128_t *)&tmp2] = _385; */
mov tmp2_0 v385;
/* _386 = MEM[(uint128_t *)&tmp2 + 16B]; */
mov v386 tmp2_16;
/* _387 = _386 * 2; */
umul v387 v386 0x2@uint128;
/* MEM[(uint128_t *)&tmp2 + 16B] = _387; */
mov tmp2_16 v387;
/* _388 = MEM[(uint128_t *)&tmp2 + 32B]; */
mov v388 tmp2_32;
/* _389 = _388 * 2; */
umul v389 v388 0x2@uint128;
/* MEM[(uint128_t *)&tmp2 + 32B] = _389; */
mov tmp2_32 v389;
/* _390 = MEM[(uint128_t *)&tmp2 + 48B]; */
mov v390 tmp2_48;
/* _391 = _390 * 2; */
umul v391 v390 0x2@uint128;
/* MEM[(uint128_t *)&tmp2 + 48B] = _391; */
mov tmp2_48 v391;
/* _392 = MEM[(uint128_t *)&tmp2 + 64B]; */
mov v392 tmp2_64;
/* _393 = _392 * 2; */
umul v393 v392 0x2@uint128;
/* MEM[(uint128_t *)&tmp2 + 64B] = _393; */
mov tmp2_64 v393;
/* _394 = MEM[(uint128_t *)&tmp2 + 80B]; */
mov v394 tmp2_80;
/* _395 = _394 * 2; */
umul v395 v394 0x2@uint128;
/* MEM[(uint128_t *)&tmp2 + 80B] = _395; */
mov tmp2_80 v395;
/* _396 = MEM[(uint128_t *)&tmp2 + 96B]; */
mov v396 tmp2_96;
/* _397 = _396 * 2; */
umul v397 v396 0x2@uint128;
/* MEM[(uint128_t *)&tmp2 + 96B] = _397; */
mov tmp2_96 v397;
/* _398 = MEM[(uint128_t *)&tmp2 + 112B]; */
mov v398 tmp2_112;
/* _399 = _398 * 2; */
umul v399 v398 0x2@uint128;
/* MEM[(uint128_t *)&tmp2 + 112B] = _399; */
mov tmp2_112 v399;
/* _400 = MEM[(uint128_t *)&tmp2 + 128B]; */
mov v400 tmp2_128;
/* _401 = _400 * 2; */
umul v401 v400 0x2@uint128;
/* MEM[(uint128_t *)&tmp2 + 128B] = _401; */
mov tmp2_128 v401;
/* felem_diff128 (&tmp, &tmp2); */
/* TODO: skipped, GIMPLE_CALL doesn't use internal or builtin function, inline function or self translte */
/* felem_reduce (&y_out, &tmp); */
/* TODO: skipped, GIMPLE_CALL doesn't use internal or builtin function, inline function or self translte */
/* vect_cst__678 = {z1_is_zero_10, z1_is_zero_10}; */
mov v678_0 z1_is_zero10;
mov v678_1 z1_is_zero10;
/* vect__220.275_814 = MEM[(const limb *)x2_32(D)]; */
mov vect__220.275814_0 x232_0;
mov vect__220.275814_8 x232_8;
/* vect__222.278_815 = MEM[(limb *)&x_out]; */
mov vect__222.278815_0 x_out_0;
mov vect__222.278815_8 x_out_8;
/* vect__223.279_816 = vect__220.275_814 ^ vect__222.278_815; */
/* TODO: Skip translation for bit xor */
/* vect_tmp_224.280_817 = vect_cst__678 & vect__223.279_816; */
/* TODO: Bitwise And/Or , may need assert and assume */
and uint64 vect_tmp_224.280817_0 v678_0 vect__223.279816_0;
and uint64 vect_tmp_224.280817_8 v678_8 vect__223.279816_8;
/* vect__225.281_818 = vect__222.278_815 ^ vect_tmp_224.280_817; */
/* TODO: Skip translation for bit xor */
/* vect__220.275_829 = MEM[(const limb *)x2_32(D) + 16B]; */
mov vect__220.275829_0 x232_16;
mov vect__220.275829_8 x232_24;
/* vect__222.278_830 = MEM[(limb *)&x_out + 16B]; */
mov vect__222.278830_0 x_out_16;
mov vect__222.278830_8 x_out_24;
/* vect__223.279_831 = vect__220.275_829 ^ vect__222.278_830; */
/* TODO: Skip translation for bit xor */
/* vect_tmp_224.280_832 = vect_cst__678 & vect__223.279_831; */
/* TODO: Bitwise And/Or , may need assert and assume */
and uint64 vect_tmp_224.280832_0 v678_0 vect__223.279831_0;
and uint64 vect_tmp_224.280832_8 v678_8 vect__223.279831_8;
/* vect__225.281_833 = vect__222.278_830 ^ vect_tmp_224.280_832; */
/* TODO: Skip translation for bit xor */
/* vect__220.275_844 = MEM[(const limb *)x2_32(D) + 32B]; */
mov vect__220.275844_0 x232_32;
mov vect__220.275844_8 x232_40;
/* vect__222.278_845 = MEM[(limb *)&x_out + 32B]; */
mov vect__222.278845_0 x_out_32;
mov vect__222.278845_8 x_out_40;
/* vect__223.279_846 = vect__220.275_844 ^ vect__222.278_845; */
/* TODO: Skip translation for bit xor */
/* vect_tmp_224.280_847 = vect_cst__678 & vect__223.279_846; */
/* TODO: Bitwise And/Or , may need assert and assume */
and uint64 vect_tmp_224.280847_0 v678_0 vect__223.279846_0;
and uint64 vect_tmp_224.280847_8 v678_8 vect__223.279846_8;
/* vect__225.281_848 = vect__222.278_845 ^ vect_tmp_224.280_847; */
/* TODO: Skip translation for bit xor */
/* vect__220.275_673 = MEM[(const limb *)x2_32(D) + 48B]; */
mov vect__220.275673_0 x232_48;
mov vect__220.275673_8 x232_56;
/* vect__222.278_676 = MEM[(limb *)&x_out + 48B]; */
mov vect__222.278676_0 x_out_48;
mov vect__222.278676_8 x_out_56;
/* vect__223.279_677 = vect__220.275_673 ^ vect__222.278_676; */
/* TODO: Skip translation for bit xor */
/* vect_tmp_224.280_679 = vect__223.279_677 & vect_cst__678; */
/* TODO: Bitwise And/Or , may need assert and assume */
and uint64 vect_tmp_224.280679_0 vect__223.279677_0 v678_0;
and uint64 vect_tmp_224.280679_8 vect__223.279677_8 v678_8;
/* vect__225.281_680 = vect__222.278_676 ^ vect_tmp_224.280_679; */
/* TODO: Skip translation for bit xor */
/* _661 = MEM[(const limb *)x2_32(D) + 64B]; */
mov v661 x232_64;
/* _663 = MEM[(limb *)&x_out + 64B]; */
mov v663 x_out_64;
/* _664 = _661 ^ _663; */
/* TODO: Skip translation for bit xor */
/* tmp_665 = z1_is_zero_10 & _664; */
/* TODO: Bitwise And/Or , may need assert and assume */
and uint64 tmp665 z1_is_zero10 v664;
/* _666 = _663 ^ tmp_665; */
/* TODO: Skip translation for bit xor */
/* vect_cst__645 = {z2_is_zero_13, z2_is_zero_13}; */
mov v645_0 z2_is_zero13;
mov v645_1 z2_is_zero13;
/* vect__209.262_769 = MEM[(const limb *)x1_19(D)]; */
mov vect__209.262769_0 x119_0;
mov vect__209.262769_8 x119_8;
/* vect__212.266_771 = vect__209.262_769 ^ vect__225.281_818; */
/* TODO: Skip translation for bit xor */
/* vect_tmp_213.267_772 = vect_cst__645 & vect__212.266_771; */
/* TODO: Bitwise And/Or , may need assert and assume */
and uint64 vect_tmp_213.267772_0 v645_0 vect__212.266771_0;
and uint64 vect_tmp_213.267772_8 v645_8 vect__212.266771_8;
/* vect__214.268_773 = vect_tmp_213.267_772 ^ vect__225.281_818; */
/* TODO: Skip translation for bit xor */
/* MEM[(limb *)&x_out] = vect__214.268_773; */
mov x_out_0 vect__214.268773_0;
mov x_out_8 vect__214.268773_8;
/* vect__209.262_784 = MEM[(const limb *)x1_19(D) + 16B]; */
mov vect__209.262784_0 x119_16;
mov vect__209.262784_8 x119_24;
/* vect__212.266_786 = vect__209.262_784 ^ vect__225.281_833; */
/* TODO: Skip translation for bit xor */
/* vect_tmp_213.267_787 = vect_cst__645 & vect__212.266_786; */
/* TODO: Bitwise And/Or , may need assert and assume */
and uint64 vect_tmp_213.267787_0 v645_0 vect__212.266786_0;
and uint64 vect_tmp_213.267787_8 v645_8 vect__212.266786_8;
/* vect__214.268_788 = vect_tmp_213.267_787 ^ vect__225.281_833; */
/* TODO: Skip translation for bit xor */
/* MEM[(limb *)&x_out + 16B] = vect__214.268_788; */
mov x_out_16 vect__214.268788_0;
mov x_out_24 vect__214.268788_8;
/* vect__209.262_799 = MEM[(const limb *)x1_19(D) + 32B]; */
mov vect__209.262799_0 x119_32;
mov vect__209.262799_8 x119_40;
/* vect__212.266_801 = vect__209.262_799 ^ vect__225.281_848; */
/* TODO: Skip translation for bit xor */
/* vect_tmp_213.267_802 = vect_cst__645 & vect__212.266_801; */
/* TODO: Bitwise And/Or , may need assert and assume */
and uint64 vect_tmp_213.267802_0 v645_0 vect__212.266801_0;
and uint64 vect_tmp_213.267802_8 v645_8 vect__212.266801_8;
/* vect__214.268_803 = vect_tmp_213.267_802 ^ vect__225.281_848; */
/* TODO: Skip translation for bit xor */
/* MEM[(limb *)&x_out + 32B] = vect__214.268_803; */
mov x_out_32 vect__214.268803_0;
mov x_out_40 vect__214.268803_8;
/* vect__209.262_640 = MEM[(const limb *)x1_19(D) + 48B]; */
mov vect__209.262640_0 x119_48;
mov vect__209.262640_8 x119_56;
/* vect__212.266_644 = vect__209.262_640 ^ vect__225.281_680; */
/* TODO: Skip translation for bit xor */
/* vect_tmp_213.267_646 = vect__212.266_644 & vect_cst__645; */
/* TODO: Bitwise And/Or , may need assert and assume */
and uint64 vect_tmp_213.267646_0 vect__212.266644_0 v645_0;
and uint64 vect_tmp_213.267646_8 vect__212.266644_8 v645_8;
/* vect__214.268_647 = vect_tmp_213.267_646 ^ vect__225.281_680; */
/* TODO: Skip translation for bit xor */
/* MEM[(limb *)&x_out + 48B] = vect__214.268_647; */
mov x_out_48 vect__214.268647_0;
mov x_out_56 vect__214.268647_8;
/* _628 = MEM[(const limb *)x1_19(D) + 64B]; */
mov v628 x119_64;
/* _631 = _628 ^ _666; */
/* TODO: Skip translation for bit xor */
/* tmp_632 = z2_is_zero_13 & _631; */
/* TODO: Bitwise And/Or , may need assert and assume */
and uint64 tmp632 z2_is_zero13 v631;
/* _633 = tmp_632 ^ _666; */
/* TODO: Skip translation for bit xor */
/* vect__198.249_724 = MEM[(const limb *)y2_42(D)]; */
mov vect__198.249724_0 y242_0;
mov vect__198.249724_8 y242_8;
/* vect__200.252_725 = MEM[(limb *)&y_out]; */
mov vect__200.252725_0 y_out_0;
mov vect__200.252725_8 y_out_8;
/* vect__201.253_726 = vect__198.249_724 ^ vect__200.252_725; */
/* TODO: Skip translation for bit xor */
/* vect_tmp_202.254_727 = vect_cst__678 & vect__201.253_726; */
/* TODO: Bitwise And/Or , may need assert and assume */
and uint64 vect_tmp_202.254727_0 v678_0 vect__201.253726_0;
and uint64 vect_tmp_202.254727_8 v678_8 vect__201.253726_8;
/* vect__203.255_728 = vect__200.252_725 ^ vect_tmp_202.254_727; */
/* TODO: Skip translation for bit xor */
/* vect__198.249_739 = MEM[(const limb *)y2_42(D) + 16B]; */
mov vect__198.249739_0 y242_16;
mov vect__198.249739_8 y242_24;
/* vect__200.252_740 = MEM[(limb *)&y_out + 16B]; */
mov vect__200.252740_0 y_out_16;
mov vect__200.252740_8 y_out_24;
/* vect__201.253_741 = vect__198.249_739 ^ vect__200.252_740; */
/* TODO: Skip translation for bit xor */
/* vect_tmp_202.254_742 = vect_cst__678 & vect__201.253_741; */
/* TODO: Bitwise And/Or , may need assert and assume */
and uint64 vect_tmp_202.254742_0 v678_0 vect__201.253741_0;
and uint64 vect_tmp_202.254742_8 v678_8 vect__201.253741_8;
/* vect__203.255_743 = vect__200.252_740 ^ vect_tmp_202.254_742; */
/* TODO: Skip translation for bit xor */
/* vect__198.249_754 = MEM[(const limb *)y2_42(D) + 32B]; */
mov vect__198.249754_0 y242_32;
mov vect__198.249754_8 y242_40;
/* vect__200.252_755 = MEM[(limb *)&y_out + 32B]; */
mov vect__200.252755_0 y_out_32;
mov vect__200.252755_8 y_out_40;
/* vect__201.253_756 = vect__198.249_754 ^ vect__200.252_755; */
/* TODO: Skip translation for bit xor */
/* vect_tmp_202.254_757 = vect_cst__678 & vect__201.253_756; */
/* TODO: Bitwise And/Or , may need assert and assume */
and uint64 vect_tmp_202.254757_0 v678_0 vect__201.253756_0;
and uint64 vect_tmp_202.254757_8 v678_8 vect__201.253756_8;
/* vect__203.255_758 = vect__200.252_755 ^ vect_tmp_202.254_757; */
/* TODO: Skip translation for bit xor */
/* vect__198.249_607 = MEM[(const limb *)y2_42(D) + 48B]; */
mov vect__198.249607_0 y242_48;
mov vect__198.249607_8 y242_56;
/* vect__200.252_610 = MEM[(limb *)&y_out + 48B]; */
mov vect__200.252610_0 y_out_48;
mov vect__200.252610_8 y_out_56;
/* vect__201.253_611 = vect__198.249_607 ^ vect__200.252_610; */
/* TODO: Skip translation for bit xor */
/* vect_tmp_202.254_613 = vect__201.253_611 & vect_cst__678; */
/* TODO: Bitwise And/Or , may need assert and assume */
and uint64 vect_tmp_202.254613_0 vect__201.253611_0 v678_0;
and uint64 vect_tmp_202.254613_8 vect__201.253611_8 v678_8;
/* vect__203.255_614 = vect__200.252_610 ^ vect_tmp_202.254_613; */
/* TODO: Skip translation for bit xor */
/* _595 = MEM[(const limb *)y2_42(D) + 64B]; */
mov v595 y242_64;
/* _597 = MEM[(limb *)&y_out + 64B]; */
mov v597 y_out_64;
/* _598 = _595 ^ _597; */
/* TODO: Skip translation for bit xor */
/* tmp_599 = z1_is_zero_10 & _598; */
/* TODO: Bitwise And/Or , may need assert and assume */
and uint64 tmp599 z1_is_zero10 v598;
/* _600 = _597 ^ tmp_599; */
/* TODO: Skip translation for bit xor */
/* vect__187.236_547 = MEM[(const limb *)y1_29(D)]; */
mov vect__187.236547_0 y129_0;
mov vect__187.236547_8 y129_8;
/* vect__190.240_279 = vect__187.236_547 ^ vect__203.255_728; */
/* TODO: Skip translation for bit xor */
/* vect_tmp_191.241_280 = vect__190.240_279 & vect_cst__645; */
/* TODO: Bitwise And/Or , may need assert and assume */
and uint64 vect_tmp_191.241280_0 vect__190.240279_0 v645_0;
and uint64 vect_tmp_191.241280_8 vect__190.240279_8 v645_8;
/* vect__192.242_281 = vect_tmp_191.241_280 ^ vect__203.255_728; */
/* TODO: Skip translation for bit xor */
/* MEM[(limb *)&y_out] = vect__192.242_281; */
mov y_out_0 vect__192.242281_0;
mov y_out_8 vect__192.242281_8;
/* vect__187.236_694 = MEM[(const limb *)y1_29(D) + 16B]; */
mov vect__187.236694_0 y129_16;
mov vect__187.236694_8 y129_24;
/* vect__190.240_696 = vect__187.236_694 ^ vect__203.255_743; */
/* TODO: Skip translation for bit xor */
/* vect_tmp_191.241_697 = vect_cst__645 & vect__190.240_696; */
/* TODO: Bitwise And/Or , may need assert and assume */
and uint64 vect_tmp_191.241697_0 v645_0 vect__190.240696_0;
and uint64 vect_tmp_191.241697_8 v645_8 vect__190.240696_8;
/* vect__192.242_698 = vect_tmp_191.241_697 ^ vect__203.255_743; */
/* TODO: Skip translation for bit xor */
/* MEM[(limb *)&y_out + 16B] = vect__192.242_698; */
mov y_out_16 vect__192.242698_0;
mov y_out_24 vect__192.242698_8;
/* vect__187.236_709 = MEM[(const limb *)y1_29(D) + 32B]; */
mov vect__187.236709_0 y129_32;
mov vect__187.236709_8 y129_40;
/* vect__190.240_711 = vect__187.236_709 ^ vect__203.255_758; */
/* TODO: Skip translation for bit xor */
/* vect_tmp_191.241_712 = vect_cst__645 & vect__190.240_711; */
/* TODO: Bitwise And/Or , may need assert and assume */
and uint64 vect_tmp_191.241712_0 v645_0 vect__190.240711_0;
and uint64 vect_tmp_191.241712_8 v645_8 vect__190.240711_8;
/* vect__192.242_713 = vect_tmp_191.241_712 ^ vect__203.255_758; */
/* TODO: Skip translation for bit xor */
/* MEM[(limb *)&y_out + 32B] = vect__192.242_713; */
mov y_out_32 vect__192.242713_0;
mov y_out_40 vect__192.242713_8;
/* vect__187.236_571 = MEM[(const limb *)y1_29(D) + 48B]; */
mov vect__187.236571_0 y129_48;
mov vect__187.236571_8 y129_56;
/* vect__190.240_565 = vect__187.236_571 ^ vect__203.255_614; */
/* TODO: Skip translation for bit xor */
/* vect_tmp_191.241_559 = vect__190.240_565 & vect_cst__645; */
/* TODO: Bitwise And/Or , may need assert and assume */
and uint64 vect_tmp_191.241559_0 vect__190.240565_0 v645_0;
and uint64 vect_tmp_191.241559_8 vect__190.240565_8 v645_8;
/* vect__192.242_555 = vect_tmp_191.241_559 ^ vect__203.255_614; */
/* TODO: Skip translation for bit xor */
/* MEM[(limb *)&y_out + 48B] = vect__192.242_555; */
mov y_out_48 vect__192.242555_0;
mov y_out_56 vect__192.242555_8;
/* _584 = MEM[(const limb *)y1_29(D) + 64B]; */
mov v584 y129_64;
/* _580 = _584 ^ _600; */
/* TODO: Skip translation for bit xor */
/* tmp_576 = z2_is_zero_13 & _580; */
/* TODO: Bitwise And/Or , may need assert and assume */
and uint64 tmp576 z2_is_zero13 v580;
/* _577 = tmp_576 ^ _600; */
/* TODO: Skip translation for bit xor */
/* vect__176.223_199 = MEM[(const limb *)z2_11(D)]; */
mov vect__176.223199_0 z211_0;
mov vect__176.223199_8 z211_8;
/* vect__178.226_200 = MEM[(limb *)&z_out]; */
mov vect__178.226200_0 z_out_0;
mov vect__178.226200_8 z_out_8;
/* vect__179.227_201 = vect__176.223_199 ^ vect__178.226_200; */
/* TODO: Skip translation for bit xor */
/* vect_tmp_180.228_202 = vect__179.227_201 & vect_cst__678; */
/* TODO: Bitwise And/Or , may need assert and assume */
and uint64 vect_tmp_180.228202_0 vect__179.227201_0 v678_0;
and uint64 vect_tmp_180.228202_8 vect__179.227201_8 v678_8;
/* vect__181.229_203 = vect__178.226_200 ^ vect_tmp_180.228_202; */
/* TODO: Skip translation for bit xor */
/* vect__176.223_192 = MEM[(const limb *)z2_11(D) + 16B]; */
mov vect__176.223192_0 z211_16;
mov vect__176.223192_8 z211_24;
/* vect__178.226_193 = MEM[(limb *)&z_out + 16B]; */
mov vect__178.226193_0 z_out_16;
mov vect__178.226193_8 z_out_24;
/* vect__179.227_574 = vect__176.223_192 ^ vect__178.226_193; */
/* TODO: Skip translation for bit xor */
/* vect_tmp_180.228_173 = vect__179.227_574 & vect_cst__678; */
/* TODO: Bitwise And/Or , may need assert and assume */
and uint64 vect_tmp_180.228173_0 vect__179.227574_0 v678_0;
and uint64 vect_tmp_180.228173_8 vect__179.227574_8 v678_8;
/* vect__181.229_174 = vect_tmp_180.228_173 ^ vect__178.226_193; */
/* TODO: Skip translation for bit xor */
/* vect__176.223_163 = MEM[(const limb *)z2_11(D) + 32B]; */
mov vect__176.223163_0 z211_32;
mov vect__176.223163_8 z211_40;
/* vect__178.226_164 = MEM[(limb *)&z_out + 32B]; */
mov vect__178.226164_0 z_out_32;
mov vect__178.226164_8 z_out_40;
/* vect__179.227_165 = vect__176.223_163 ^ vect__178.226_164; */
/* TODO: Skip translation for bit xor */
/* vect_tmp_180.228_166 = vect__179.227_165 & vect_cst__678; */
/* TODO: Bitwise And/Or , may need assert and assume */
and uint64 vect_tmp_180.228166_0 vect__179.227165_0 v678_0;
and uint64 vect_tmp_180.228166_8 vect__179.227165_8 v678_8;
/* vect__181.229_167 = vect__178.226_164 ^ vect_tmp_180.228_166; */
/* TODO: Skip translation for bit xor */
/* vect__176.223_262 = MEM[(const limb *)z2_11(D) + 48B]; */
mov vect__176.223262_0 z211_48;
mov vect__176.223262_8 z211_56;
/* vect__178.226_250 = MEM[(limb *)&z_out + 48B]; */
mov vect__178.226250_0 z_out_48;
mov vect__178.226250_8 z_out_56;
/* vect__179.227_249 = vect__178.226_250 ^ vect__176.223_262; */
/* TODO: Skip translation for bit xor */
/* vect_tmp_180.228_247 = vect__179.227_249 & vect_cst__678; */
/* TODO: Bitwise And/Or , may need assert and assume */
and uint64 vect_tmp_180.228247_0 vect__179.227249_0 v678_0;
and uint64 vect_tmp_180.228247_8 vect__179.227249_8 v678_8;
/* vect__181.229_246 = vect_tmp_180.228_247 ^ vect__178.226_250; */
/* TODO: Skip translation for bit xor */
/* _353 = MEM[(const limb *)z2_11(D) + 64B]; */
mov v353 z211_64;
/* _347 = MEM[(limb *)&z_out + 64B]; */
mov v347 z_out_64;
/* _344 = _347 ^ _353; */
/* TODO: Skip translation for bit xor */
/* tmp_341 = z1_is_zero_10 & _344; */
/* TODO: Bitwise And/Or , may need assert and assume */
and uint64 tmp341 z1_is_zero10 v344;
/* _338 = tmp_341 ^ _347; */
/* TODO: Skip translation for bit xor */
/* vect__165.210_265 = MEM[(const limb *)z1_8(D)]; */
mov vect__165.210265_0 z18_0;
mov vect__165.210265_8 z18_8;
/* vect__168.214_22 = vect__181.229_203 ^ vect__165.210_265; */
/* TODO: Skip translation for bit xor */
/* vect_tmp_169.215_161 = vect__168.214_22 & vect_cst__645; */
/* TODO: Bitwise And/Or , may need assert and assume */
and uint64 vect_tmp_169.215161_0 vect__168.21422_0 v645_0;
and uint64 vect_tmp_169.215161_8 vect__168.21422_8 v645_8;
/* vect__170.216_569 = vect_tmp_169.215_161 ^ vect__181.229_203; */
/* TODO: Skip translation for bit xor */
/* MEM[(limb *)&z_out] = vect__170.216_569; */
mov z_out_0 vect__170.216569_0;
mov z_out_8 vect__170.216569_8;
/* vect__165.210_654 = MEM[(const limb *)z1_8(D) + 16B]; */
mov vect__165.210654_0 z18_16;
mov vect__165.210654_8 z18_24;
/* vect__168.214_554 = vect__181.229_174 ^ vect__165.210_654; */
/* TODO: Skip translation for bit xor */
/* vect_tmp_169.215_531 = vect__168.214_554 & vect_cst__645; */
/* TODO: Bitwise And/Or , may need assert and assume */
and uint64 vect_tmp_169.215531_0 vect__168.214554_0 v645_0;
and uint64 vect_tmp_169.215531_8 vect__168.214554_8 v645_8;
/* vect__170.216_217 = vect__181.229_174 ^ vect_tmp_169.215_531; */
/* TODO: Skip translation for bit xor */
/* MEM[(limb *)&z_out + 16B] = vect__170.216_217; */
mov z_out_16 vect__170.216217_0;
mov z_out_24 vect__170.216217_8;
/* vect__165.210_206 = MEM[(const limb *)z1_8(D) + 32B]; */
mov vect__165.210206_0 z18_32;
mov vect__165.210206_8 z18_40;
/* vect__168.214_208 = vect__181.229_167 ^ vect__165.210_206; */
/* TODO: Skip translation for bit xor */
/* vect_tmp_169.215_209 = vect__168.214_208 & vect_cst__645; */
/* TODO: Bitwise And/Or , may need assert and assume */
and uint64 vect_tmp_169.215209_0 vect__168.214208_0 v645_0;
and uint64 vect_tmp_169.215209_8 vect__168.214208_8 v645_8;
/* vect__170.216_210 = vect__181.229_167 ^ vect_tmp_169.215_209; */
/* TODO: Skip translation for bit xor */
/* MEM[(limb *)&z_out + 32B] = vect__170.216_210; */
mov z_out_32 vect__170.216210_0;
mov z_out_40 vect__170.216210_8;
/* vect__165.210_452 = MEM[(const limb *)z1_8(D) + 48B]; */
mov vect__165.210452_0 z18_48;
mov vect__165.210452_8 z18_56;
/* vect__168.214_444 = vect__181.229_246 ^ vect__165.210_452; */
/* TODO: Skip translation for bit xor */
/* vect_tmp_169.215_440 = vect__168.214_444 & vect_cst__645; */
/* TODO: Bitwise And/Or , may need assert and assume */
and uint64 vect_tmp_169.215440_0 vect__168.214444_0 v645_0;
and uint64 vect_tmp_169.215440_8 vect__168.214444_8 v645_8;
/* vect__170.216_438 = vect__181.229_246 ^ vect_tmp_169.215_440; */
/* TODO: Skip translation for bit xor */
/* MEM[(limb *)&z_out + 48B] = vect__170.216_438; */
mov z_out_48 vect__170.216438_0;
mov z_out_56 vect__170.216438_8;
/* _539 = MEM[(const limb *)z1_8(D) + 64B]; */
mov v539 z18_64;
/* _470 = _338 ^ _539; */
/* TODO: Skip translation for bit xor */
/* tmp_468 = z2_is_zero_13 & _470; */
/* TODO: Bitwise And/Or , may need assert and assume */
and uint64 tmp468 z2_is_zero13 v470;
/* _466 = _338 ^ tmp_468; */
/* TODO: Skip translation for bit xor */
/* _152 = MEM[(const limb *)&x_out]; */
mov v152 x_out_0;
/* *x3_49(D) = _152; */
mov x349_0 v152;
/* _153 = MEM[(const limb *)&x_out + 8B]; */
mov v153 x_out_8;
/* MEM[(limb *)x3_49(D) + 8B] = _153; */
mov x349_8 v153;
/* _154 = MEM[(const limb *)&x_out + 16B]; */
mov v154 x_out_16;
/* MEM[(limb *)x3_49(D) + 16B] = _154; */
mov x349_16 v154;
/* _155 = MEM[(const limb *)&x_out + 24B]; */
mov v155 x_out_24;
/* MEM[(limb *)x3_49(D) + 24B] = _155; */
mov x349_24 v155;
/* _156 = MEM[(const limb *)&x_out + 32B]; */
mov v156 x_out_32;
/* MEM[(limb *)x3_49(D) + 32B] = _156; */
mov x349_32 v156;
/* _157 = MEM[(const limb *)&x_out + 40B]; */
mov v157 x_out_40;
/* MEM[(limb *)x3_49(D) + 40B] = _157; */
mov x349_40 v157;
/* _158 = MEM[(const limb *)&x_out + 48B]; */
mov v158 x_out_48;
/* MEM[(limb *)x3_49(D) + 48B] = _158; */
mov x349_48 v158;
/* _159 = MEM[(const limb *)&x_out + 56B]; */
mov v159 x_out_56;
/* MEM[(limb *)x3_49(D) + 56B] = _159; */
mov x349_56 v159;
/* MEM[(limb *)x3_49(D) + 64B] = _633; */
mov x349_64 v633;
/* _143 = MEM[(const limb *)&y_out]; */
mov v143 y_out_0;
/* *y3_50(D) = _143; */
mov y350_0 v143;
/* _144 = MEM[(const limb *)&y_out + 8B]; */
mov v144 y_out_8;
/* MEM[(limb *)y3_50(D) + 8B] = _144; */
mov y350_8 v144;
/* _145 = MEM[(const limb *)&y_out + 16B]; */
mov v145 y_out_16;
/* MEM[(limb *)y3_50(D) + 16B] = _145; */
mov y350_16 v145;
/* _146 = MEM[(const limb *)&y_out + 24B]; */
mov v146 y_out_24;
/* MEM[(limb *)y3_50(D) + 24B] = _146; */
mov y350_24 v146;
/* _147 = MEM[(const limb *)&y_out + 32B]; */
mov v147 y_out_32;
/* MEM[(limb *)y3_50(D) + 32B] = _147; */
mov y350_32 v147;
/* _148 = MEM[(const limb *)&y_out + 40B]; */
mov v148 y_out_40;
/* MEM[(limb *)y3_50(D) + 40B] = _148; */
mov y350_40 v148;
/* _149 = MEM[(const limb *)&y_out + 48B]; */
mov v149 y_out_48;
/* MEM[(limb *)y3_50(D) + 48B] = _149; */
mov y350_48 v149;
/* _150 = MEM[(const limb *)&y_out + 56B]; */
mov v150 y_out_56;
/* MEM[(limb *)y3_50(D) + 56B] = _150; */
mov y350_56 v150;
/* MEM[(limb *)y3_50(D) + 64B] = _577; */
mov y350_64 v577;
/* _134 = MEM[(const limb *)&z_out]; */
mov v134 z_out_0;
/* *z3_51(D) = _134; */
mov z351_0 v134;
/* _135 = MEM[(const limb *)&z_out + 8B]; */
mov v135 z_out_8;
/* MEM[(limb *)z3_51(D) + 8B] = _135; */
mov z351_8 v135;
/* _136 = MEM[(const limb *)&z_out + 16B]; */
mov v136 z_out_16;
/* MEM[(limb *)z3_51(D) + 16B] = _136; */
mov z351_16 v136;
/* _137 = MEM[(const limb *)&z_out + 24B]; */
mov v137 z_out_24;
/* MEM[(limb *)z3_51(D) + 24B] = _137; */
mov z351_24 v137;
/* _138 = MEM[(const limb *)&z_out + 32B]; */
mov v138 z_out_32;
/* MEM[(limb *)z3_51(D) + 32B] = _138; */
mov z351_32 v138;
/* _139 = MEM[(const limb *)&z_out + 40B]; */
mov v139 z_out_40;
/* MEM[(limb *)z3_51(D) + 40B] = _139; */
mov z351_40 v139;
/* _140 = MEM[(const limb *)&z_out + 48B]; */
mov v140 z_out_48;
/* MEM[(limb *)z3_51(D) + 48B] = _140; */
mov z351_48 v140;
/* _141 = MEM[(const limb *)&z_out + 56B]; */
mov v141 z_out_56;
/* MEM[(limb *)z3_51(D) + 56B] = _141; */
mov z351_56 v141;
/* MEM[(limb *)z3_51(D) + 64B] = _466; */
mov z351_64 v466;
/* ftmp ={v} {CLOBBER}; */
/* TODO: Skip translation for constructor */
/* ftmp2 ={v} {CLOBBER}; */
/* TODO: Skip translation for constructor */
/* ftmp3 ={v} {CLOBBER}; */
/* TODO: Skip translation for constructor */
/* ftmp4 ={v} {CLOBBER}; */
/* TODO: Skip translation for constructor */
/* ftmp5 ={v} {CLOBBER}; */
/* TODO: Skip translation for constructor */
/* ftmp6 ={v} {CLOBBER}; */
/* TODO: Skip translation for constructor */
/* x_out ={v} {CLOBBER}; */
/* TODO: Skip translation for constructor */
/* y_out ={v} {CLOBBER}; */
/* TODO: Skip translation for constructor */
/* z_out ={v} {CLOBBER}; */
/* TODO: Skip translation for constructor */
/* tmp ={v} {CLOBBER}; */
/* TODO: Skip translation for constructor */
/* tmp2 ={v} {CLOBBER}; */
/* TODO: Skip translation for constructor */
/* goto <bb 9> */
/* TODO: unconditional jump */

/* BB's index is 9 */

/* .MEM_6 = PHI <.MEM_92(8), .MEM_63(7)> */
/* return; */


/* Start with unused lhs */
mov _ ftmp_0@uint64;
mov _ ftmp_8@uint64;
mov _ ftmp_16@uint64;
mov _ ftmp_24@uint64;
mov _ ftmp_32@uint64;
mov _ ftmp_40@uint64;
mov _ ftmp_48@uint64;
mov _ ftmp_56@uint64;
mov _ ftmp_64@uint64;
mov _ ftmp3_0@uint64;
mov _ ftmp3_8@uint64;
mov _ ftmp3_16@uint64;
mov _ ftmp3_24@uint64;
mov _ ftmp3_32@uint64;
mov _ ftmp3_40@uint64;
mov _ ftmp3_48@uint64;
mov _ ftmp3_56@uint64;
mov _ ftmp3_64@uint64;
mov _ ftmp4_0@uint64;
mov _ ftmp4_8@uint64;
mov _ ftmp4_16@uint64;
mov _ ftmp4_24@uint64;
mov _ ftmp4_32@uint64;
mov _ ftmp4_40@uint64;
mov _ ftmp4_48@uint64;
mov _ ftmp4_56@uint64;
mov _ ftmp4_64@uint64;
mov _ ftmp5_0@uint64;
mov _ ftmp5_8@uint64;
mov _ ftmp5_16@uint64;
mov _ ftmp5_24@uint64;
mov _ ftmp5_32@uint64;
mov _ ftmp5_40@uint64;
mov _ ftmp5_48@uint64;
mov _ ftmp5_56@uint64;
mov _ ftmp5_64@uint64;
mov _ ftmp6_0@uint64;
mov _ ftmp6_8@uint64;
mov _ ftmp6_16@uint64;
mov _ ftmp6_24@uint64;
mov _ ftmp6_32@uint64;
mov _ ftmp6_40@uint64;
mov _ ftmp6_48@uint64;
mov _ ftmp6_56@uint64;
mov _ ftmp6_64@uint64;
mov _ tmp2_0@uint128;
mov _ tmp2_16@uint128;
mov _ tmp2_32@uint128;
mov _ tmp2_48@uint128;
mov _ tmp2_64@uint128;
mov _ tmp2_80@uint128;
mov _ tmp2_96@uint128;
mov _ tmp2_112@uint128;
mov _ tmp2_128@uint128;
mov _ tmp341@uint64;
mov _ tmp468@uint64;
mov _ tmp576@uint64;
mov _ tmp599@uint64;
mov _ tmp632@uint64;
mov _ tmp665@uint64;
mov _ v3@uint8;
mov _ v347@uint64;
mov _ v353@uint64;
mov _ v4@uint64;
mov _ v539@uint64;
mov _ v584@uint64;
mov _ v595@uint64;
mov _ v597@uint64;
mov _ v628@uint64;
mov _ v645_0;
mov _ v645_1;
mov _ v661@uint64;
mov _ v663@uint64;
mov _ v678_0;
mov _ v678_1;
mov _ vect__165.210206_0;
mov _ vect__165.210206_8;
mov _ vect__165.210265_0;
mov _ vect__165.210265_8;
mov _ vect__165.210452_0;
mov _ vect__165.210452_8;
mov _ vect__165.210654_0;
mov _ vect__165.210654_8;
mov _ vect__176.223163_0;
mov _ vect__176.223163_8;
mov _ vect__176.223192_0;
mov _ vect__176.223192_8;
mov _ vect__176.223199_0;
mov _ vect__176.223199_8;
mov _ vect__176.223262_0;
mov _ vect__176.223262_8;
mov _ vect__178.226164_0;
mov _ vect__178.226164_8;
mov _ vect__178.226193_0;
mov _ vect__178.226193_8;
mov _ vect__178.226200_0;
mov _ vect__178.226200_8;
mov _ vect__178.226250_0;
mov _ vect__178.226250_8;
mov _ vect__187.236547_0;
mov _ vect__187.236547_8;
mov _ vect__187.236571_0;
mov _ vect__187.236571_8;
mov _ vect__187.236694_0;
mov _ vect__187.236694_8;
mov _ vect__187.236709_0;
mov _ vect__187.236709_8;
mov _ vect__198.249607_0;
mov _ vect__198.249607_8;
mov _ vect__198.249724_0;
mov _ vect__198.249724_8;
mov _ vect__198.249739_0;
mov _ vect__198.249739_8;
mov _ vect__198.249754_0;
mov _ vect__198.249754_8;
mov _ vect__200.252610_0;
mov _ vect__200.252610_8;
mov _ vect__200.252725_0;
mov _ vect__200.252725_8;
mov _ vect__200.252740_0;
mov _ vect__200.252740_8;
mov _ vect__200.252755_0;
mov _ vect__200.252755_8;
mov _ vect__209.262640_0;
mov _ vect__209.262640_8;
mov _ vect__209.262769_0;
mov _ vect__209.262769_8;
mov _ vect__209.262784_0;
mov _ vect__209.262784_8;
mov _ vect__209.262799_0;
mov _ vect__209.262799_8;
mov _ vect__220.275673_0;
mov _ vect__220.275673_8;
mov _ vect__220.275814_0;
mov _ vect__220.275814_8;
mov _ vect__220.275829_0;
mov _ vect__220.275829_8;
mov _ vect__220.275844_0;
mov _ vect__220.275844_8;
mov _ vect__222.278676_0;
mov _ vect__222.278676_8;
mov _ vect__222.278815_0;
mov _ vect__222.278815_8;
mov _ vect__222.278830_0;
mov _ vect__222.278830_8;
mov _ vect__222.278845_0;
mov _ vect__222.278845_8;
mov _ vect_tmp_169.215161_0;
mov _ vect_tmp_169.215161_8;
mov _ vect_tmp_169.215209_0;
mov _ vect_tmp_169.215209_8;
mov _ vect_tmp_169.215440_0;
mov _ vect_tmp_169.215440_8;
mov _ vect_tmp_169.215531_0;
mov _ vect_tmp_169.215531_8;
mov _ vect_tmp_180.228166_0;
mov _ vect_tmp_180.228166_8;
mov _ vect_tmp_180.228173_0;
mov _ vect_tmp_180.228173_8;
mov _ vect_tmp_180.228202_0;
mov _ vect_tmp_180.228202_8;
mov _ vect_tmp_180.228247_0;
mov _ vect_tmp_180.228247_8;
mov _ vect_tmp_191.241280_0;
mov _ vect_tmp_191.241280_8;
mov _ vect_tmp_191.241559_0;
mov _ vect_tmp_191.241559_8;
mov _ vect_tmp_191.241697_0;
mov _ vect_tmp_191.241697_8;
mov _ vect_tmp_191.241712_0;
mov _ vect_tmp_191.241712_8;
mov _ vect_tmp_202.254613_0;
mov _ vect_tmp_202.254613_8;
mov _ vect_tmp_202.254727_0;
mov _ vect_tmp_202.254727_8;
mov _ vect_tmp_202.254742_0;
mov _ vect_tmp_202.254742_8;
mov _ vect_tmp_202.254757_0;
mov _ vect_tmp_202.254757_8;
mov _ vect_tmp_213.267646_0;
mov _ vect_tmp_213.267646_8;
mov _ vect_tmp_213.267772_0;
mov _ vect_tmp_213.267772_8;
mov _ vect_tmp_213.267787_0;
mov _ vect_tmp_213.267787_8;
mov _ vect_tmp_213.267802_0;
mov _ vect_tmp_213.267802_8;
mov _ vect_tmp_224.280679_0;
mov _ vect_tmp_224.280679_8;
mov _ vect_tmp_224.280817_0;
mov _ vect_tmp_224.280817_8;
mov _ vect_tmp_224.280832_0;
mov _ vect_tmp_224.280832_8;
mov _ vect_tmp_224.280847_0;
mov _ vect_tmp_224.280847_8;
mov _ x349_0@uint64;
mov _ x349_8@uint64;
mov _ x349_16@uint64;
mov _ x349_24@uint64;
mov _ x349_32@uint64;
mov _ x349_40@uint64;
mov _ x349_48@uint64;
mov _ x349_56@uint64;
mov _ x349_64@uint64;
mov _ y350_0@uint64;
mov _ y350_8@uint64;
mov _ y350_16@uint64;
mov _ y350_24@uint64;
mov _ y350_32@uint64;
mov _ y350_40@uint64;
mov _ y350_48@uint64;
mov _ y350_56@uint64;
mov _ y350_64@uint64;
mov _ z351_0@uint64;
mov _ z351_8@uint64;
mov _ z351_16@uint64;
mov _ z351_24@uint64;
mov _ z351_32@uint64;
mov _ z351_40@uint64;
mov _ z351_48@uint64;
mov _ z351_56@uint64;
mov _ z351_64@uint64;
/* End with unsed lhs */


{
  true
  &&
  true
}
